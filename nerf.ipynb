{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFSynthetic(Dataset):\n",
    "    def __init__(self, data_path: str, split=\"train\", img_size=(800, 800), transform: transforms = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (str): Path to the dataset folder (e.g., \"nerf_synthetic/chair\").\n",
    "            split (str): Dataset split to load (\"train\", \"val\", or \"test\").\n",
    "            img_size (tuple): Target size of the images.\n",
    "            transform (transforms): Optional transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "\n",
    "        json_path = os.path.join(data_path, f\"transforms_{split}.json\")\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.meta = json.load(f)\n",
    "\n",
    "        # Extract image file paths and corresponding poses\n",
    "        self.image_paths = [os.path.join(data_path, frame[\"file_path\"] + \".png\") for frame in self.meta[\"frames\"]]\n",
    "        self.poses = [np.array(frame[\"transform_matrix\"], dtype=np.float32) for frame in self.meta[\"frames\"]]\n",
    "\n",
    "        # Assume focal length is the same for all images\n",
    "        # Add intrinsics extraction if available in your dataset\n",
    "        self.focal_length = self.meta[\"frames\"][0][\"intrinsics\"][0] if \"intrinsics\" in self.meta[\"frames\"][0] else None\n",
    "\n",
    "         # Default image transform pipeline if not provided\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(self.img_size),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image and apply transformations\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Get the corresponding pose\n",
    "        pose = torch.tensor(self.poses[idx], dtype=torch.float32)\n",
    "\n",
    "        # Return focal length if available\n",
    "        if self.focal_length is not None:\n",
    "            focal_length = torch.tensor(self.focal_length, dtype=torch.float32)\n",
    "            return img, pose, focal_length\n",
    "        else:\n",
    "            return img, pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerf_chair_train_set = NeRFSynthetic(\"/Users/rickypramanick/Desktop/nerf/nerf_synthetic/chair\")\n",
    "len(nerf_chair_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 800, 800])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerf_chair_train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerf_chair_train_set[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9250,  0.2749, -0.2623, -1.0572],\n",
       "        [-0.3799, -0.6693,  0.6385,  2.5740],\n",
       "        [ 0.0000,  0.6903,  0.7235,  2.9166],\n",
       "        [ 0.0000,  0.0000,  0.0000,  1.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerf_chair_train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
